{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd013b019e3a74db624fff6f1107034c3f47c3d751b0aa318cdcc8979410004c4f5",
   "display_name": "Python 3.7.10 64-bit ('py37': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "13b019e3a74db624fff6f1107034c3f47c3d751b0aa318cdcc8979410004c4f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "HW02\n",
    "Deep Learning, GIST RT5101-01, 2021, Spring, (Tue/Thurs 2:30~3:45)\n",
    "***\n",
    "\n",
    "\n",
    "How to submit your homework\n",
    "Submit your jupyter notebook file with the filename of HW02_studentnumber.ipynb on GEL\n",
    "Ex) HW02_20184021.ipynb\n",
    "\n",
    "Submission deadline\n",
    "2021.05.12, Wednesday 23:59 (PM)\n",
    "\n",
    "Plagiarism\n",
    "We encourage you to discuss this homework with your friends or TA, but you should write your own code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Problem 1. (Build simple fully connected layer model)\n",
    "- step 1. Build your simple FCN layer by using fully connected layer\n",
    "- step 2. train your own model\n",
    "- Do not use coding lecture sample model connected"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleFCN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=3*32*32, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc4 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.out_fc = nn.Linear(in_features=64, out_features=num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        output = self.out_fc(x)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "source": [
    "### Problem 2. (dataset code part)\n",
    "- Load your dataset from dataset folder\n",
    "- using rgb dataset ex) cifar10, cifar100\n",
    "- you can use custom dataset from web site ex) dog vs cat"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "def dataset(is_train):\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    dataset = datasets.CIFAR10(root='./dataset', train=is_train, transform=transform, download=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "source": [
    "## Model train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 0\n",
    "lr = 1e-4\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_dataset = dataset(is_train=True)\n",
    "val_dataset = dataset(is_train=False)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "                                                        \n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\n",
    "\n",
    "model = SimpleFCN(num_classes=10).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "source": [
    "train_loss_arr = []\n",
    "train_acc_arr = []\n",
    "\n",
    "val_loss_arr = []\n",
    "val_acc_arr = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data) \n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        output.float()\n",
    "        loss.float()\n",
    "\n",
    "        prec1 = accuracy(output.data, target)\n",
    "        prec1 = prec1[0]\n",
    "\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(prec1.item(), data.size(0))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(epoch, i, len(train_loader), loss=losses))\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss_arr.append(losses.avg)\n",
    "    train_acc_arr.append(top1.avg)\n",
    "    print(\"train result: Loss: {}, Acc: {}\\n\".format(losses.avg, top1.avg))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_sum = 0\n",
    "        val_acc_sum = 0\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        for i, (data, target) in enumerate(val_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data) \n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            output.float()\n",
    "            loss.float()\n",
    "\n",
    "            prec1 = accuracy(output.data, target)\n",
    "\n",
    "            prec1 = prec1[0]\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(prec1.item(), data.size(0))\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(val_loader), loss=losses, top1=top1))\n",
    "\n",
    "        val_loss_arr.append(losses.avg)\n",
    "        val_acc_arr.append(top1.avg)\n",
    "        print(\"Validation result: Loss: {}, Acc: {}\\n\".format(losses.avg, top1.avg))\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: [0][0/782]\tLoss 2.3281 (2.3281)\n",
      "Epoch: [0][100/782]\tLoss 2.3019 (2.3074)\n",
      "Epoch: [0][200/782]\tLoss 2.2896 (2.3066)\n",
      "Epoch: [0][300/782]\tLoss 2.2953 (2.3066)\n",
      "Epoch: [0][400/782]\tLoss 2.3095 (2.3065)\n",
      "Epoch: [0][500/782]\tLoss 2.2977 (2.3070)\n",
      "Epoch: [0][600/782]\tLoss 2.3233 (2.3065)\n",
      "Epoch: [0][700/782]\tLoss 2.3039 (2.3062)\n",
      "train result: Loss: 2.3062437857818603, Acc: 10.198\n",
      "\n",
      "Test: [0/157]\tLoss 2.3023 (2.3023)\tPrec@1 9.375 (9.375)\n",
      "Test: [100/157]\tLoss 2.3165 (2.3049)\tPrec@1 9.375 (10.334)\n",
      "Validation result: Loss: 2.304970682144165, Acc: 10.3\n",
      "\n",
      "Epoch: [1][0/782]\tLoss 2.3078 (2.3078)\n",
      "Epoch: [1][100/782]\tLoss 2.3057 (2.3057)\n",
      "Epoch: [1][200/782]\tLoss 2.3012 (2.3054)\n",
      "Epoch: [1][300/782]\tLoss 2.2961 (2.3050)\n",
      "Epoch: [1][400/782]\tLoss 2.2976 (2.3042)\n",
      "Epoch: [1][500/782]\tLoss 2.3319 (2.3040)\n",
      "Epoch: [1][600/782]\tLoss 2.2971 (2.3040)\n",
      "Epoch: [1][700/782]\tLoss 2.3092 (2.3039)\n",
      "train result: Loss: 2.3037633891296387, Acc: 11.126\n",
      "\n",
      "Test: [0/157]\tLoss 2.2998 (2.2998)\tPrec@1 12.500 (12.500)\n",
      "Test: [100/157]\tLoss 2.3132 (2.3026)\tPrec@1 12.500 (12.531)\n",
      "Validation result: Loss: 2.302656128692627, Acc: 12.45\n",
      "\n",
      "Epoch: [2][0/782]\tLoss 2.3116 (2.3116)\n",
      "Epoch: [2][100/782]\tLoss 2.3118 (2.3010)\n",
      "Epoch: [2][200/782]\tLoss 2.3044 (2.3010)\n",
      "Epoch: [2][300/782]\tLoss 2.3019 (2.3013)\n",
      "Epoch: [2][400/782]\tLoss 2.3008 (2.3013)\n",
      "Epoch: [2][500/782]\tLoss 2.2999 (2.3013)\n",
      "Epoch: [2][600/782]\tLoss 2.3170 (2.3013)\n",
      "Epoch: [2][700/782]\tLoss 2.3011 (2.3015)\n",
      "train result: Loss: 2.301514822540283, Acc: 13.07\n",
      "\n",
      "Test: [0/157]\tLoss 2.2970 (2.2970)\tPrec@1 17.188 (17.188)\n",
      "Test: [100/157]\tLoss 2.3103 (2.3004)\tPrec@1 12.500 (14.264)\n",
      "Validation result: Loss: 2.3003849292755127, Acc: 14.27\n",
      "\n",
      "Epoch: [3][0/782]\tLoss 2.3015 (2.3015)\n",
      "Epoch: [3][100/782]\tLoss 2.2920 (2.2987)\n",
      "Epoch: [3][200/782]\tLoss 2.3082 (2.2997)\n",
      "Epoch: [3][300/782]\tLoss 2.2928 (2.2995)\n",
      "Epoch: [3][400/782]\tLoss 2.3056 (2.2997)\n",
      "Epoch: [3][500/782]\tLoss 2.3059 (2.2994)\n",
      "Epoch: [3][600/782]\tLoss 2.2761 (2.2991)\n",
      "Epoch: [3][700/782]\tLoss 2.2886 (2.2992)\n",
      "train result: Loss: 2.2991826544189453, Acc: 14.394\n",
      "\n",
      "Test: [0/157]\tLoss 2.2939 (2.2939)\tPrec@1 17.188 (17.188)\n",
      "Test: [100/157]\tLoss 2.3074 (2.2979)\tPrec@1 14.062 (14.867)\n",
      "Validation result: Loss: 2.297951469039917, Acc: 14.94\n",
      "\n",
      "Epoch: [4][0/782]\tLoss 2.3062 (2.3062)\n",
      "Epoch: [4][100/782]\tLoss 2.3124 (2.2979)\n",
      "Epoch: [4][200/782]\tLoss 2.2835 (2.2965)\n",
      "Epoch: [4][300/782]\tLoss 2.3130 (2.2965)\n",
      "Epoch: [4][400/782]\tLoss 2.2908 (2.2967)\n",
      "Epoch: [4][500/782]\tLoss 2.3054 (2.2968)\n",
      "Epoch: [4][600/782]\tLoss 2.2956 (2.2969)\n",
      "Epoch: [4][700/782]\tLoss 2.2962 (2.2967)\n",
      "train result: Loss: 2.296646015548706, Acc: 14.666\n",
      "\n",
      "Test: [0/157]\tLoss 2.2905 (2.2905)\tPrec@1 15.625 (15.625)\n",
      "Test: [100/157]\tLoss 2.3042 (2.2952)\tPrec@1 14.062 (14.898)\n",
      "Validation result: Loss: 2.2952560680389404, Acc: 15.05\n",
      "\n",
      "Epoch: [5][0/782]\tLoss 2.3061 (2.3061)\n",
      "Epoch: [5][100/782]\tLoss 2.3018 (2.2948)\n",
      "Epoch: [5][200/782]\tLoss 2.2940 (2.2950)\n",
      "Epoch: [5][300/782]\tLoss 2.2940 (2.2949)\n",
      "Epoch: [5][400/782]\tLoss 2.2980 (2.2947)\n",
      "Epoch: [5][500/782]\tLoss 2.2908 (2.2944)\n",
      "Epoch: [5][600/782]\tLoss 2.2868 (2.2943)\n",
      "Epoch: [5][700/782]\tLoss 2.2819 (2.2940)\n",
      "train result: Loss: 2.293771710586548, Acc: 14.7\n",
      "\n",
      "Test: [0/157]\tLoss 2.2864 (2.2864)\tPrec@1 17.188 (17.188)\n",
      "Test: [100/157]\tLoss 2.3008 (2.2921)\tPrec@1 15.625 (14.867)\n",
      "Validation result: Loss: 2.292119696807861, Acc: 15.14\n",
      "\n",
      "Epoch: [6][0/782]\tLoss 2.2977 (2.2977)\n",
      "Epoch: [6][100/782]\tLoss 2.3014 (2.2921)\n",
      "Epoch: [6][200/782]\tLoss 2.2837 (2.2915)\n",
      "Epoch: [6][300/782]\tLoss 2.2793 (2.2916)\n",
      "Epoch: [6][400/782]\tLoss 2.2919 (2.2912)\n",
      "Epoch: [6][500/782]\tLoss 2.2913 (2.2911)\n",
      "Epoch: [6][600/782]\tLoss 2.2904 (2.2907)\n",
      "Epoch: [6][700/782]\tLoss 2.2860 (2.2905)\n",
      "train result: Loss: 2.2903319286346435, Acc: 14.85\n",
      "\n",
      "Test: [0/157]\tLoss 2.2811 (2.2811)\tPrec@1 15.625 (15.625)\n",
      "Test: [100/157]\tLoss 2.2968 (2.2882)\tPrec@1 17.188 (14.975)\n",
      "Validation result: Loss: 2.288246167373657, Acc: 15.22\n",
      "\n",
      "Epoch: [7][0/782]\tLoss 2.2845 (2.2845)\n",
      "Epoch: [7][100/782]\tLoss 2.2914 (2.2891)\n",
      "Epoch: [7][200/782]\tLoss 2.2846 (2.2882)\n",
      "Epoch: [7][300/782]\tLoss 2.2833 (2.2877)\n",
      "Epoch: [7][400/782]\tLoss 2.2813 (2.2873)\n",
      "Epoch: [7][500/782]\tLoss 2.2861 (2.2870)\n",
      "Epoch: [7][600/782]\tLoss 2.2760 (2.2866)\n",
      "Epoch: [7][700/782]\tLoss 2.2844 (2.2861)\n",
      "train result: Loss: 2.285966015625, Acc: 15.108\n",
      "\n",
      "Test: [0/157]\tLoss 2.2739 (2.2739)\tPrec@1 14.062 (14.062)\n",
      "Test: [100/157]\tLoss 2.2917 (2.2832)\tPrec@1 15.625 (15.176)\n",
      "Validation result: Loss: 2.2831829536437986, Acc: 15.59\n",
      "\n",
      "Epoch: [8][0/782]\tLoss 2.2974 (2.2974)\n",
      "Epoch: [8][100/782]\tLoss 2.2798 (2.2831)\n",
      "Epoch: [8][200/782]\tLoss 2.2813 (2.2829)\n",
      "Epoch: [8][300/782]\tLoss 2.2871 (2.2824)\n",
      "Epoch: [8][400/782]\tLoss 2.2735 (2.2815)\n",
      "Epoch: [8][500/782]\tLoss 2.2674 (2.2811)\n",
      "Epoch: [8][600/782]\tLoss 2.2775 (2.2809)\n",
      "Epoch: [8][700/782]\tLoss 2.2759 (2.2806)\n",
      "train result: Loss: 2.280150305633545, Acc: 15.46\n",
      "\n",
      "Test: [0/157]\tLoss 2.2639 (2.2639)\tPrec@1 17.188 (17.188)\n",
      "Test: [100/157]\tLoss 2.2850 (2.2763)\tPrec@1 14.062 (15.269)\n",
      "Validation result: Loss: 2.276342147064209, Acc: 15.69\n",
      "\n",
      "Epoch: [9][0/782]\tLoss 2.2846 (2.2846)\n",
      "Epoch: [9][100/782]\tLoss 2.2690 (2.2755)\n",
      "Epoch: [9][200/782]\tLoss 2.2755 (2.2756)\n",
      "Epoch: [9][300/782]\tLoss 2.2678 (2.2750)\n",
      "Epoch: [9][400/782]\tLoss 2.2656 (2.2743)\n",
      "Epoch: [9][500/782]\tLoss 2.2753 (2.2737)\n",
      "Epoch: [9][600/782]\tLoss 2.2708 (2.2731)\n",
      "Epoch: [9][700/782]\tLoss 2.2574 (2.2725)\n",
      "train result: Loss: 2.2721749312591553, Acc: 16.66\n",
      "\n",
      "Test: [0/157]\tLoss 2.2495 (2.2495)\tPrec@1 23.438 (23.438)\n",
      "Test: [100/157]\tLoss 2.2761 (2.2669)\tPrec@1 20.312 (17.543)\n",
      "Validation result: Loss: 2.2668551517486573, Acc: 17.78\n",
      "\n",
      "Epoch: [10][0/782]\tLoss 2.2775 (2.2775)\n",
      "Epoch: [10][100/782]\tLoss 2.2674 (2.2657)\n",
      "Epoch: [10][200/782]\tLoss 2.2620 (2.2661)\n",
      "Epoch: [10][300/782]\tLoss 2.2490 (2.2651)\n",
      "Epoch: [10][400/782]\tLoss 2.2604 (2.2640)\n",
      "Epoch: [10][500/782]\tLoss 2.2395 (2.2632)\n",
      "Epoch: [10][600/782]\tLoss 2.2571 (2.2626)\n",
      "Epoch: [10][700/782]\tLoss 2.2599 (2.2619)\n",
      "train result: Loss: 2.2611295933532713, Acc: 18.928\n",
      "\n",
      "Test: [0/157]\tLoss 2.2289 (2.2289)\tPrec@1 28.125 (28.125)\n",
      "Test: [100/157]\tLoss 2.2641 (2.2537)\tPrec@1 18.750 (18.796)\n",
      "Validation result: Loss: 2.253674661254883, Acc: 19.22\n",
      "\n",
      "Epoch: [11][0/782]\tLoss 2.2705 (2.2705)\n",
      "Epoch: [11][100/782]\tLoss 2.2319 (2.2528)\n",
      "Epoch: [11][200/782]\tLoss 2.2639 (2.2520)\n",
      "Epoch: [11][300/782]\tLoss 2.2667 (2.2508)\n",
      "Epoch: [11][400/782]\tLoss 2.2373 (2.2504)\n",
      "Epoch: [11][500/782]\tLoss 2.2492 (2.2492)\n",
      "Epoch: [11][600/782]\tLoss 2.2086 (2.2480)\n",
      "Epoch: [11][700/782]\tLoss 2.2729 (2.2473)\n",
      "train result: Loss: 2.2460631233215334, Acc: 19.354\n",
      "\n",
      "Test: [0/157]\tLoss 2.2006 (2.2006)\tPrec@1 31.250 (31.250)\n",
      "Test: [100/157]\tLoss 2.2486 (2.2360)\tPrec@1 20.312 (19.013)\n",
      "Validation result: Loss: 2.235952946853638, Acc: 19.23\n",
      "\n",
      "Epoch: [12][0/782]\tLoss 2.2409 (2.2409)\n",
      "Epoch: [12][100/782]\tLoss 2.2442 (2.2336)\n",
      "Epoch: [12][200/782]\tLoss 2.2224 (2.2338)\n",
      "Epoch: [12][300/782]\tLoss 2.2413 (2.2324)\n",
      "Epoch: [12][400/782]\tLoss 2.2704 (2.2319)\n",
      "Epoch: [12][500/782]\tLoss 2.1972 (2.2302)\n",
      "Epoch: [12][600/782]\tLoss 2.2417 (2.2292)\n",
      "Epoch: [12][700/782]\tLoss 2.2129 (2.2278)\n",
      "train result: Loss: 2.2266638725280763, Acc: 19.174\n",
      "\n",
      "Test: [0/157]\tLoss 2.1648 (2.1648)\tPrec@1 32.812 (32.812)\n",
      "Test: [100/157]\tLoss 2.2301 (2.2140)\tPrec@1 18.750 (19.508)\n",
      "Validation result: Loss: 2.213921421432495, Acc: 19.66\n",
      "\n",
      "Epoch: [13][0/782]\tLoss 2.1772 (2.1772)\n",
      "Epoch: [13][100/782]\tLoss 2.2579 (2.2135)\n",
      "Epoch: [13][200/782]\tLoss 2.1794 (2.2116)\n",
      "Epoch: [13][300/782]\tLoss 2.2043 (2.2091)\n",
      "Epoch: [13][400/782]\tLoss 2.1837 (2.2081)\n",
      "Epoch: [13][500/782]\tLoss 2.2225 (2.2073)\n",
      "Epoch: [13][600/782]\tLoss 2.1616 (2.2064)\n",
      "Epoch: [13][700/782]\tLoss 2.1937 (2.2049)\n",
      "train result: Loss: 2.2037795357513428, Acc: 19.398\n",
      "\n",
      "Test: [0/157]\tLoss 2.1248 (2.1248)\tPrec@1 29.688 (29.688)\n",
      "Test: [100/157]\tLoss 2.2105 (2.1889)\tPrec@1 17.188 (20.297)\n",
      "Validation result: Loss: 2.188762725067139, Acc: 20.5\n",
      "\n",
      "Epoch: [14][0/782]\tLoss 2.2038 (2.2038)\n",
      "Epoch: [14][100/782]\tLoss 2.1102 (2.1881)\n",
      "Epoch: [14][200/782]\tLoss 2.1981 (2.1887)\n",
      "Epoch: [14][300/782]\tLoss 2.0952 (2.1872)\n",
      "Epoch: [14][400/782]\tLoss 2.1764 (2.1845)\n",
      "Epoch: [14][500/782]\tLoss 2.1416 (2.1831)\n",
      "Epoch: [14][600/782]\tLoss 2.1403 (2.1807)\n",
      "Epoch: [14][700/782]\tLoss 2.1169 (2.1794)\n",
      "train result: Loss: 2.177918243484497, Acc: 20.632\n",
      "\n",
      "Test: [0/157]\tLoss 2.0826 (2.0826)\tPrec@1 31.250 (31.250)\n",
      "Test: [100/157]\tLoss 2.1912 (2.1609)\tPrec@1 18.750 (21.705)\n",
      "Validation result: Loss: 2.1604909183502197, Acc: 21.82\n",
      "\n",
      "Epoch: [15][0/782]\tLoss 2.1408 (2.1408)\n",
      "Epoch: [15][100/782]\tLoss 2.1829 (2.1613)\n",
      "Epoch: [15][200/782]\tLoss 2.1543 (2.1596)\n",
      "Epoch: [15][300/782]\tLoss 2.1122 (2.1588)\n",
      "Epoch: [15][400/782]\tLoss 2.1576 (2.1569)\n",
      "Epoch: [15][500/782]\tLoss 2.1119 (2.1541)\n",
      "Epoch: [15][600/782]\tLoss 2.1803 (2.1518)\n",
      "Epoch: [15][700/782]\tLoss 2.1643 (2.1502)\n",
      "train result: Loss: 2.1491679381561277, Acc: 21.358\n",
      "\n",
      "Test: [0/157]\tLoss 2.0407 (2.0407)\tPrec@1 32.812 (32.812)\n",
      "Test: [100/157]\tLoss 2.1732 (2.1313)\tPrec@1 21.875 (22.184)\n",
      "Validation result: Loss: 2.130276704406738, Acc: 22.41\n",
      "\n",
      "Epoch: [16][0/782]\tLoss 2.1197 (2.1197)\n",
      "Epoch: [16][100/782]\tLoss 2.1887 (2.1307)\n",
      "Epoch: [16][200/782]\tLoss 2.0798 (2.1301)\n",
      "Epoch: [16][300/782]\tLoss 2.0718 (2.1275)\n",
      "Epoch: [16][400/782]\tLoss 2.2226 (2.1271)\n",
      "Epoch: [16][500/782]\tLoss 2.0427 (2.1245)\n",
      "Epoch: [16][600/782]\tLoss 2.0922 (2.1226)\n",
      "Epoch: [16][700/782]\tLoss 2.0716 (2.1219)\n",
      "train result: Loss: 2.119795071105957, Acc: 21.93\n",
      "\n",
      "Test: [0/157]\tLoss 2.0032 (2.0032)\tPrec@1 34.375 (34.375)\n",
      "Test: [100/157]\tLoss 2.1578 (2.1029)\tPrec@1 21.875 (22.850)\n",
      "Validation result: Loss: 2.101224485397339, Acc: 22.8\n",
      "\n",
      "Epoch: [17][0/782]\tLoss 2.1282 (2.1282)\n",
      "Epoch: [17][100/782]\tLoss 2.0550 (2.1017)\n",
      "Epoch: [17][200/782]\tLoss 2.0465 (2.0985)\n",
      "Epoch: [17][300/782]\tLoss 2.0676 (2.0990)\n",
      "Epoch: [17][400/782]\tLoss 2.0987 (2.0978)\n",
      "Epoch: [17][500/782]\tLoss 2.0617 (2.0964)\n",
      "Epoch: [17][600/782]\tLoss 2.1693 (2.0949)\n",
      "Epoch: [17][700/782]\tLoss 2.1436 (2.0932)\n",
      "train result: Loss: 2.0926317009735107, Acc: 22.558\n",
      "\n",
      "Test: [0/157]\tLoss 1.9745 (1.9745)\tPrec@1 35.938 (35.938)\n",
      "Test: [100/157]\tLoss 2.1404 (2.0775)\tPrec@1 23.438 (23.809)\n",
      "Validation result: Loss: 2.0751103038787844, Acc: 23.64\n",
      "\n",
      "Epoch: [18][0/782]\tLoss 2.1655 (2.1655)\n",
      "Epoch: [18][100/782]\tLoss 2.0863 (2.0746)\n",
      "Epoch: [18][200/782]\tLoss 1.9878 (2.0713)\n",
      "Epoch: [18][300/782]\tLoss 2.0843 (2.0721)\n",
      "Epoch: [18][400/782]\tLoss 2.0767 (2.0721)\n",
      "Epoch: [18][500/782]\tLoss 2.1071 (2.0710)\n",
      "Epoch: [18][600/782]\tLoss 1.9975 (2.0712)\n",
      "Epoch: [18][700/782]\tLoss 2.0270 (2.0705)\n",
      "train result: Loss: 2.0685928119659422, Acc: 23.446\n",
      "\n",
      "Test: [0/157]\tLoss 1.9535 (1.9535)\tPrec@1 39.062 (39.062)\n",
      "Test: [100/157]\tLoss 2.1195 (2.0553)\tPrec@1 26.562 (24.783)\n",
      "Validation result: Loss: 2.052202910041809, Acc: 24.71\n",
      "\n",
      "Epoch: [19][0/782]\tLoss 2.0404 (2.0404)\n",
      "Epoch: [19][100/782]\tLoss 2.0301 (2.0521)\n",
      "Epoch: [19][200/782]\tLoss 1.9941 (2.0497)\n",
      "Epoch: [19][300/782]\tLoss 2.0454 (2.0482)\n",
      "Epoch: [19][400/782]\tLoss 1.9849 (2.0498)\n",
      "Epoch: [19][500/782]\tLoss 2.0267 (2.0499)\n",
      "Epoch: [19][600/782]\tLoss 2.0183 (2.0489)\n",
      "Epoch: [19][700/782]\tLoss 2.0225 (2.0484)\n",
      "train result: Loss: 2.0472619613647463, Acc: 24.622\n",
      "\n",
      "Test: [0/157]\tLoss 1.9345 (1.9345)\tPrec@1 37.500 (37.500)\n",
      "Test: [100/157]\tLoss 2.0963 (2.0352)\tPrec@1 26.562 (25.851)\n",
      "Validation result: Loss: 2.0315357999801638, Acc: 25.79\n",
      "\n"
     ]
    }
   ]
  },
  {
   "source": [
    "### Problem 3. (Build CNN model with fc layer)\n",
    "- step 1. Build your simple CNN layer by using convolutional layer and fully connected layer\n",
    "- step 2. train your own model\n",
    "- Do not use coding lecture sample model connected"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(5,5)) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5,5))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=5*5*16, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)        \n",
    "        self.out_fc = nn.Linear(in_features=84, out_features=num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)               \n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        output = self.out_fc(x)        \n",
    "\n",
    "        return output"
   ]
  },
  {
   "source": [
    "### Problem 4. (Compare FCN model and CNN model)\n",
    "- compare two model's performance\n",
    "- subscribe your comment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0\n",
    "lr = 1e-4\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\"\"\"\n",
    "train_dataset = dataset(is_train=True)\n",
    "val_dataset = dataset(is_train=False)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "                                                        \n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\"\"\"\n",
    "model = CNN_model(num_classes=10).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: [0][0/782]\tLoss 2.3013 (2.3013)\n",
      "Epoch: [0][100/782]\tLoss 2.3063 (2.3036)\n",
      "Epoch: [0][200/782]\tLoss 2.2984 (2.3040)\n",
      "Epoch: [0][300/782]\tLoss 2.3019 (2.3037)\n",
      "Epoch: [0][400/782]\tLoss 2.2949 (2.3038)\n",
      "Epoch: [0][500/782]\tLoss 2.3004 (2.3037)\n",
      "Epoch: [0][600/782]\tLoss 2.3084 (2.3036)\n",
      "Epoch: [0][700/782]\tLoss 2.3033 (2.3034)\n",
      "train result: Loss: 2.3035147527313233, Acc: 10.204\n",
      "\n",
      "Test: [0/157]\tLoss 2.3013 (2.3013)\tPrec@1 12.500 (12.500)\n",
      "Test: [100/157]\tLoss 2.3068 (2.3031)\tPrec@1 6.250 (9.994)\n",
      "Validation result: Loss: 2.3029784851074218, Acc: 10.11\n",
      "\n",
      "Epoch: [1][0/782]\tLoss 2.3054 (2.3054)\n",
      "Epoch: [1][100/782]\tLoss 2.2970 (2.3023)\n",
      "Epoch: [1][200/782]\tLoss 2.3008 (2.3025)\n",
      "Epoch: [1][300/782]\tLoss 2.2990 (2.3026)\n",
      "Epoch: [1][400/782]\tLoss 2.3053 (2.3027)\n",
      "Epoch: [1][500/782]\tLoss 2.2978 (2.3026)\n",
      "Epoch: [1][600/782]\tLoss 2.2972 (2.3025)\n",
      "Epoch: [1][700/782]\tLoss 2.2978 (2.3025)\n",
      "train result: Loss: 2.3024975106811523, Acc: 10.284\n",
      "\n",
      "Test: [0/157]\tLoss 2.3000 (2.3000)\tPrec@1 12.500 (12.500)\n",
      "Test: [100/157]\tLoss 2.3056 (2.3020)\tPrec@1 7.812 (10.195)\n",
      "Validation result: Loss: 2.301928323364258, Acc: 10.18\n",
      "\n",
      "Epoch: [2][0/782]\tLoss 2.3086 (2.3086)\n",
      "Epoch: [2][100/782]\tLoss 2.3077 (2.3022)\n",
      "Epoch: [2][200/782]\tLoss 2.3106 (2.3018)\n",
      "Epoch: [2][300/782]\tLoss 2.3134 (2.3016)\n",
      "Epoch: [2][400/782]\tLoss 2.3005 (2.3015)\n",
      "Epoch: [2][500/782]\tLoss 2.3018 (2.3015)\n",
      "Epoch: [2][600/782]\tLoss 2.3025 (2.3016)\n",
      "Epoch: [2][700/782]\tLoss 2.3046 (2.3016)\n",
      "train result: Loss: 2.30141704284668, Acc: 10.384\n",
      "\n",
      "Test: [0/157]\tLoss 2.2986 (2.2986)\tPrec@1 12.500 (12.500)\n",
      "Test: [100/157]\tLoss 2.3045 (2.3009)\tPrec@1 7.812 (10.288)\n",
      "Validation result: Loss: 2.3007602111816405, Acc: 10.29\n",
      "\n",
      "Epoch: [3][0/782]\tLoss 2.3015 (2.3015)\n",
      "Epoch: [3][100/782]\tLoss 2.3049 (2.3003)\n",
      "Epoch: [3][200/782]\tLoss 2.3043 (2.3002)\n",
      "Epoch: [3][300/782]\tLoss 2.3080 (2.3006)\n",
      "Epoch: [3][400/782]\tLoss 2.3007 (2.3005)\n",
      "Epoch: [3][500/782]\tLoss 2.2980 (2.3003)\n",
      "Epoch: [3][600/782]\tLoss 2.3043 (2.3002)\n",
      "Epoch: [3][700/782]\tLoss 2.3042 (2.3002)\n",
      "train result: Loss: 2.3001543267059326, Acc: 10.574\n",
      "\n",
      "Test: [0/157]\tLoss 2.2969 (2.2969)\tPrec@1 12.500 (12.500)\n",
      "Test: [100/157]\tLoss 2.3033 (2.2994)\tPrec@1 7.812 (10.412)\n",
      "Validation result: Loss: 2.2993168121337892, Acc: 10.39\n",
      "\n",
      "Epoch: [4][0/782]\tLoss 2.3007 (2.3007)\n",
      "Epoch: [4][100/782]\tLoss 2.2999 (2.2987)\n",
      "Epoch: [4][200/782]\tLoss 2.3046 (2.2991)\n",
      "Epoch: [4][300/782]\tLoss 2.3022 (2.2989)\n",
      "Epoch: [4][400/782]\tLoss 2.3089 (2.2988)\n",
      "Epoch: [4][500/782]\tLoss 2.3047 (2.2988)\n",
      "Epoch: [4][600/782]\tLoss 2.3010 (2.2987)\n",
      "Epoch: [4][700/782]\tLoss 2.2953 (2.2986)\n",
      "train result: Loss: 2.2985740731048585, Acc: 10.7\n",
      "\n",
      "Test: [0/157]\tLoss 2.2946 (2.2946)\tPrec@1 12.500 (12.500)\n",
      "Test: [100/157]\tLoss 2.3018 (2.2976)\tPrec@1 6.250 (10.535)\n",
      "Validation result: Loss: 2.2974870697021483, Acc: 10.48\n",
      "\n",
      "Epoch: [5][0/782]\tLoss 2.2959 (2.2959)\n",
      "Epoch: [5][100/782]\tLoss 2.2995 (2.2978)\n",
      "Epoch: [5][200/782]\tLoss 2.2977 (2.2973)\n",
      "Epoch: [5][300/782]\tLoss 2.3008 (2.2969)\n",
      "Epoch: [5][400/782]\tLoss 2.2985 (2.2969)\n",
      "Epoch: [5][500/782]\tLoss 2.2942 (2.2968)\n",
      "Epoch: [5][600/782]\tLoss 2.3003 (2.2967)\n",
      "Epoch: [5][700/782]\tLoss 2.3030 (2.2967)\n",
      "train result: Loss: 2.296582428970337, Acc: 10.878\n",
      "\n",
      "Test: [0/157]\tLoss 2.2915 (2.2915)\tPrec@1 12.500 (12.500)\n",
      "Test: [100/157]\tLoss 2.2999 (2.2953)\tPrec@1 4.688 (10.783)\n",
      "Validation result: Loss: 2.295164046859741, Acc: 10.8\n",
      "\n",
      "Epoch: [6][0/782]\tLoss 2.3087 (2.3087)\n",
      "Epoch: [6][100/782]\tLoss 2.3023 (2.2951)\n",
      "Epoch: [6][200/782]\tLoss 2.2870 (2.2949)\n",
      "Epoch: [6][300/782]\tLoss 2.2978 (2.2950)\n",
      "Epoch: [6][400/782]\tLoss 2.2967 (2.2948)\n",
      "Epoch: [6][500/782]\tLoss 2.3002 (2.2945)\n",
      "Epoch: [6][600/782]\tLoss 2.2975 (2.2942)\n",
      "Epoch: [6][700/782]\tLoss 2.2917 (2.2940)\n",
      "train result: Loss: 2.2939737049102784, Acc: 11.586\n",
      "\n",
      "Test: [0/157]\tLoss 2.2870 (2.2870)\tPrec@1 18.750 (18.750)\n",
      "Test: [100/157]\tLoss 2.2975 (2.2921)\tPrec@1 7.812 (12.252)\n",
      "Validation result: Loss: 2.291995276260376, Acc: 12.36\n",
      "\n",
      "Epoch: [7][0/782]\tLoss 2.2941 (2.2941)\n",
      "Epoch: [7][100/782]\tLoss 2.2966 (2.2918)\n",
      "Epoch: [7][200/782]\tLoss 2.2974 (2.2917)\n",
      "Epoch: [7][300/782]\tLoss 2.2981 (2.2914)\n",
      "Epoch: [7][400/782]\tLoss 2.2924 (2.2913)\n",
      "Epoch: [7][500/782]\tLoss 2.2857 (2.2910)\n",
      "Epoch: [7][600/782]\tLoss 2.2883 (2.2908)\n",
      "Epoch: [7][700/782]\tLoss 2.2838 (2.2905)\n",
      "train result: Loss: 2.2902669827270508, Acc: 14.082\n",
      "\n",
      "Test: [0/157]\tLoss 2.2801 (2.2801)\tPrec@1 23.438 (23.438)\n",
      "Test: [100/157]\tLoss 2.2940 (2.2874)\tPrec@1 14.062 (14.898)\n",
      "Validation result: Loss: 2.287342014312744, Acc: 15.04\n",
      "\n",
      "Epoch: [8][0/782]\tLoss 2.2930 (2.2930)\n",
      "Epoch: [8][100/782]\tLoss 2.2929 (2.2869)\n",
      "Epoch: [8][200/782]\tLoss 2.2853 (2.2868)\n",
      "Epoch: [8][300/782]\tLoss 2.2991 (2.2867)\n",
      "Epoch: [8][400/782]\tLoss 2.2913 (2.2863)\n",
      "Epoch: [8][500/782]\tLoss 2.2826 (2.2858)\n",
      "Epoch: [8][600/782]\tLoss 2.2657 (2.2853)\n",
      "Epoch: [8][700/782]\tLoss 2.2931 (2.2850)\n",
      "train result: Loss: 2.2846627361297607, Acc: 15.66\n",
      "\n",
      "Test: [0/157]\tLoss 2.2694 (2.2694)\tPrec@1 25.000 (25.000)\n",
      "Test: [100/157]\tLoss 2.2886 (2.2802)\tPrec@1 12.500 (15.656)\n",
      "Validation result: Loss: 2.2801143882751465, Acc: 15.8\n",
      "\n",
      "Epoch: [9][0/782]\tLoss 2.2853 (2.2853)\n",
      "Epoch: [9][100/782]\tLoss 2.2739 (2.2811)\n",
      "Epoch: [9][200/782]\tLoss 2.2870 (2.2799)\n",
      "Epoch: [9][300/782]\tLoss 2.2900 (2.2796)\n",
      "Epoch: [9][400/782]\tLoss 2.2841 (2.2788)\n",
      "Epoch: [9][500/782]\tLoss 2.2598 (2.2779)\n",
      "Epoch: [9][600/782]\tLoss 2.2802 (2.2775)\n",
      "Epoch: [9][700/782]\tLoss 2.2698 (2.2765)\n",
      "train result: Loss: 2.2757462965393067, Acc: 16.06\n",
      "\n",
      "Test: [0/157]\tLoss 2.2512 (2.2512)\tPrec@1 29.688 (29.688)\n",
      "Test: [100/157]\tLoss 2.2802 (2.2684)\tPrec@1 14.062 (16.182)\n",
      "Validation result: Loss: 2.268395041656494, Acc: 16.21\n",
      "\n",
      "Epoch: [10][0/782]\tLoss 2.2629 (2.2629)\n",
      "Epoch: [10][100/782]\tLoss 2.2630 (2.2707)\n",
      "Epoch: [10][200/782]\tLoss 2.2729 (2.2668)\n",
      "Epoch: [10][300/782]\tLoss 2.2709 (2.2665)\n",
      "Epoch: [10][400/782]\tLoss 2.2539 (2.2658)\n",
      "Epoch: [10][500/782]\tLoss 2.2551 (2.2645)\n",
      "Epoch: [10][600/782]\tLoss 2.2624 (2.2636)\n",
      "Epoch: [10][700/782]\tLoss 2.2524 (2.2624)\n",
      "train result: Loss: 2.261465594482422, Acc: 16.672\n",
      "\n",
      "Test: [0/157]\tLoss 2.2232 (2.2232)\tPrec@1 28.125 (28.125)\n",
      "Test: [100/157]\tLoss 2.2677 (2.2500)\tPrec@1 14.062 (17.729)\n",
      "Validation result: Loss: 2.2500153770446776, Acc: 17.64\n",
      "\n",
      "Epoch: [11][0/782]\tLoss 2.2285 (2.2285)\n",
      "Epoch: [11][100/782]\tLoss 2.2514 (2.2520)\n",
      "Epoch: [11][200/782]\tLoss 2.2612 (2.2490)\n",
      "Epoch: [11][300/782]\tLoss 2.2451 (2.2472)\n",
      "Epoch: [11][400/782]\tLoss 2.2482 (2.2459)\n",
      "Epoch: [11][500/782]\tLoss 2.2579 (2.2442)\n",
      "Epoch: [11][600/782]\tLoss 2.2360 (2.2425)\n",
      "Epoch: [11][700/782]\tLoss 2.2406 (2.2409)\n",
      "train result: Loss: 2.239371778717041, Acc: 18.146\n",
      "\n",
      "Test: [0/157]\tLoss 2.1808 (2.1808)\tPrec@1 32.812 (32.812)\n",
      "Test: [100/157]\tLoss 2.2495 (2.2220)\tPrec@1 14.062 (19.059)\n",
      "Validation result: Loss: 2.2219873432159423, Acc: 18.95\n",
      "\n",
      "Epoch: [12][0/782]\tLoss 2.2998 (2.2998)\n",
      "Epoch: [12][100/782]\tLoss 2.2207 (2.2220)\n",
      "Epoch: [12][200/782]\tLoss 2.2307 (2.2203)\n",
      "Epoch: [12][300/782]\tLoss 2.2473 (2.2178)\n",
      "Epoch: [12][400/782]\tLoss 2.1839 (2.2157)\n",
      "Epoch: [12][500/782]\tLoss 2.2402 (2.2127)\n",
      "Epoch: [12][600/782]\tLoss 2.2051 (2.2108)\n",
      "Epoch: [12][700/782]\tLoss 2.2293 (2.2089)\n",
      "train result: Loss: 2.2059626485443116, Acc: 19.802\n",
      "\n",
      "Test: [0/157]\tLoss 2.1183 (2.1183)\tPrec@1 34.375 (34.375)\n",
      "Test: [100/157]\tLoss 2.2244 (2.1811)\tPrec@1 20.312 (20.483)\n",
      "Validation result: Loss: 2.1805730709075926, Acc: 20.48\n",
      "\n",
      "Epoch: [13][0/782]\tLoss 2.1855 (2.1855)\n",
      "Epoch: [13][100/782]\tLoss 2.1765 (2.1848)\n",
      "Epoch: [13][200/782]\tLoss 2.2300 (2.1800)\n",
      "Epoch: [13][300/782]\tLoss 2.1115 (2.1756)\n",
      "Epoch: [13][400/782]\tLoss 2.1467 (2.1744)\n",
      "Epoch: [13][500/782]\tLoss 2.1861 (2.1701)\n",
      "Epoch: [13][600/782]\tLoss 2.1445 (2.1665)\n",
      "Epoch: [13][700/782]\tLoss 2.2282 (2.1622)\n",
      "train result: Loss: 2.15951989692688, Acc: 21.234\n",
      "\n",
      "Test: [0/157]\tLoss 2.0424 (2.0424)\tPrec@1 37.500 (37.500)\n",
      "Test: [100/157]\tLoss 2.1931 (2.1301)\tPrec@1 18.750 (21.736)\n",
      "Validation result: Loss: 2.128261170196533, Acc: 21.94\n",
      "\n",
      "Epoch: [14][0/782]\tLoss 2.2128 (2.2128)\n",
      "Epoch: [14][100/782]\tLoss 2.1997 (2.1252)\n",
      "Epoch: [14][200/782]\tLoss 2.1313 (2.1255)\n",
      "Epoch: [14][300/782]\tLoss 2.1296 (2.1225)\n",
      "Epoch: [14][400/782]\tLoss 2.1373 (2.1210)\n",
      "Epoch: [14][500/782]\tLoss 2.0786 (2.1179)\n",
      "Epoch: [14][600/782]\tLoss 2.0482 (2.1126)\n",
      "Epoch: [14][700/782]\tLoss 2.1373 (2.1113)\n",
      "train result: Loss: 2.109262274017334, Acc: 22.934\n",
      "\n",
      "Test: [0/157]\tLoss 1.9862 (1.9862)\tPrec@1 40.625 (40.625)\n",
      "Test: [100/157]\tLoss 2.1666 (2.0848)\tPrec@1 17.188 (23.422)\n",
      "Validation result: Loss: 2.0808007780075073, Acc: 23.99\n",
      "\n",
      "Epoch: [15][0/782]\tLoss 2.0360 (2.0360)\n",
      "Epoch: [15][100/782]\tLoss 2.1188 (2.0863)\n",
      "Epoch: [15][200/782]\tLoss 2.0967 (2.0805)\n",
      "Epoch: [15][300/782]\tLoss 2.0800 (2.0752)\n",
      "Epoch: [15][400/782]\tLoss 1.9501 (2.0736)\n",
      "Epoch: [15][500/782]\tLoss 2.0837 (2.0712)\n",
      "Epoch: [15][600/782]\tLoss 2.0770 (2.0673)\n",
      "Epoch: [15][700/782]\tLoss 2.1923 (2.0649)\n",
      "train result: Loss: 2.0635439516830445, Acc: 25.022\n",
      "\n",
      "Test: [0/157]\tLoss 1.9405 (1.9405)\tPrec@1 42.188 (42.188)\n",
      "Test: [100/157]\tLoss 2.1238 (2.0398)\tPrec@1 17.188 (26.052)\n",
      "Validation result: Loss: 2.034257800102234, Acc: 26.34\n",
      "\n",
      "Epoch: [16][0/782]\tLoss 2.1572 (2.1572)\n",
      "Epoch: [16][100/782]\tLoss 2.0140 (2.0404)\n",
      "Epoch: [16][200/782]\tLoss 1.9606 (2.0302)\n",
      "Epoch: [16][300/782]\tLoss 1.9219 (2.0295)\n",
      "Epoch: [16][400/782]\tLoss 1.9375 (2.0272)\n",
      "Epoch: [16][500/782]\tLoss 1.9541 (2.0241)\n",
      "Epoch: [16][600/782]\tLoss 1.9590 (2.0232)\n",
      "Epoch: [16][700/782]\tLoss 1.9400 (2.0201)\n",
      "train result: Loss: 2.0184716761016848, Acc: 26.942\n",
      "\n",
      "Test: [0/157]\tLoss 1.9046 (1.9046)\tPrec@1 43.750 (43.750)\n",
      "Test: [100/157]\tLoss 2.0780 (1.9988)\tPrec@1 25.000 (28.125)\n",
      "Validation result: Loss: 1.9924848752975464, Acc: 28.3\n",
      "\n",
      "Epoch: [17][0/782]\tLoss 1.8637 (1.8637)\n",
      "Epoch: [17][100/782]\tLoss 2.0752 (2.0010)\n",
      "Epoch: [17][200/782]\tLoss 1.9847 (1.9880)\n",
      "Epoch: [17][300/782]\tLoss 1.8571 (1.9893)\n",
      "Epoch: [17][400/782]\tLoss 1.8739 (1.9858)\n",
      "Epoch: [17][500/782]\tLoss 2.0732 (1.9854)\n",
      "Epoch: [17][600/782]\tLoss 2.0033 (1.9849)\n",
      "Epoch: [17][700/782]\tLoss 1.9337 (1.9821)\n",
      "train result: Loss: 1.982091975631714, Acc: 28.188\n",
      "\n",
      "Test: [0/157]\tLoss 1.8732 (1.8732)\tPrec@1 42.188 (42.188)\n",
      "Test: [100/157]\tLoss 2.0544 (1.9678)\tPrec@1 23.438 (29.162)\n",
      "Validation result: Loss: 1.961297332572937, Acc: 29.28\n",
      "\n",
      "Epoch: [18][0/782]\tLoss 2.0176 (2.0176)\n",
      "Epoch: [18][100/782]\tLoss 2.0502 (1.9543)\n",
      "Epoch: [18][200/782]\tLoss 2.1207 (1.9590)\n",
      "Epoch: [18][300/782]\tLoss 1.9234 (1.9535)\n",
      "Epoch: [18][400/782]\tLoss 1.9353 (1.9553)\n",
      "Epoch: [18][500/782]\tLoss 2.0504 (1.9571)\n",
      "Epoch: [18][600/782]\tLoss 1.9615 (1.9569)\n",
      "Epoch: [18][700/782]\tLoss 1.8961 (1.9559)\n",
      "train result: Loss: 1.9545288722991943, Acc: 29.216\n",
      "\n",
      "Test: [0/157]\tLoss 1.8594 (1.8594)\tPrec@1 42.188 (42.188)\n",
      "Test: [100/157]\tLoss 2.0286 (1.9413)\tPrec@1 26.562 (30.121)\n",
      "Validation result: Loss: 1.9351504009246827, Acc: 30.32\n",
      "\n",
      "Epoch: [19][0/782]\tLoss 1.8938 (1.8938)\n",
      "Epoch: [19][100/782]\tLoss 1.9854 (1.9405)\n",
      "Epoch: [19][200/782]\tLoss 2.0016 (1.9398)\n",
      "Epoch: [19][300/782]\tLoss 2.1126 (1.9400)\n",
      "Epoch: [19][400/782]\tLoss 1.9356 (1.9381)\n",
      "Epoch: [19][500/782]\tLoss 1.9651 (1.9345)\n",
      "Epoch: [19][600/782]\tLoss 1.9410 (1.9354)\n",
      "Epoch: [19][700/782]\tLoss 1.9745 (1.9309)\n",
      "train result: Loss: 1.9306394415283203, Acc: 30.168\n",
      "\n",
      "Test: [0/157]\tLoss 1.8275 (1.8275)\tPrec@1 43.750 (43.750)\n",
      "Test: [100/157]\tLoss 2.0185 (1.9180)\tPrec@1 21.875 (30.693)\n",
      "Validation result: Loss: 1.912327798652649, Acc: 30.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_arr = []\n",
    "train_acc_arr = []\n",
    "\n",
    "val_loss_arr = []\n",
    "val_acc_arr = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data) \n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        output.float()\n",
    "        loss.float()\n",
    "\n",
    "        prec1 = accuracy(output.data, target)\n",
    "        prec1 = prec1[0]\n",
    "\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(prec1.item(), data.size(0))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(epoch, i, len(train_loader), loss=losses))\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss_arr.append(losses.avg)\n",
    "    train_acc_arr.append(top1.avg)\n",
    "    print(\"train result: Loss: {}, Acc: {}\\n\".format(losses.avg, top1.avg))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_sum = 0\n",
    "        val_acc_sum = 0\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        for i, (data, target) in enumerate(val_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data) \n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            output.float()\n",
    "            loss.float()\n",
    "\n",
    "            prec1 = accuracy(output.data, target)\n",
    "\n",
    "            prec1 = prec1[0]\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(prec1.item(), data.size(0))\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(val_loader), loss=losses, top1=top1))\n",
    "\n",
    "        val_loss_arr.append(losses.avg)\n",
    "        val_acc_arr.append(top1.avg)\n",
    "        print(\"Validation result: Loss: {}, Acc: {}\\n\".format(losses.avg, top1.avg))"
   ]
  },
  {
   "source": [
    "## FCN\n",
    "Test Loss : 2.0315357999801638\n",
    "Test Accuracy : 25.79\n",
    "\n",
    "## CNN\n",
    "Test Loss : 1.912327798652649\n",
    "Test Accuracy : 30.76\n",
    "\n",
    "I expected CNN models to perform significantly better than FCN models. But the actual results were slightly better. More research is needed on what methods should be used to achieve better results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}